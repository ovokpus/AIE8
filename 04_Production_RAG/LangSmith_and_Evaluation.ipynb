{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      },
      "source": [
        "# LangSmith and Evaluation Overview with AI Makerspace\n",
        "\n",
        "Today we'll be looking at an amazing tool:\n",
        "\n",
        "[LangSmith](https://docs.smith.langchain.com/)!\n",
        "\n",
        "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
        "\n",
        "We'll also be looking at a few Advanced Retrieval techniques along the way - and evaluate it using LangSmith!\n",
        "\n",
        "‚úãBREAKOUT ROOM #2:\n",
        "- Task 1: Dependencies and OpenAI API Key\n",
        "- Task 2: LangGraph RAG\n",
        "- Task 3: Setting Up LangSmith\n",
        "- Task 4: Examining the Trace in LangSmith!\n",
        "- Task 5: Create Testing Dataset\n",
        "- Task 6: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5ok9p-XuUs"
      },
      "source": [
        "## Task 1: Dependencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for our simple RAG system that leverages Loand Complaint data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADl8-whIAUHD",
        "outputId": "927fd78b-8510-4bea-9e68-a3c74d3eb5a1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923xinz42sWV"
      },
      "source": [
        "#### Asyncio Bug Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m9U0SbQN2sWc"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2AOb-QHwJm"
      },
      "source": [
        "## Task #2: Create a Simple RAG Application Using LangGraph\n",
        "\n",
        "Let's remake our LangGraph RAG pipeline from the first notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJB3IxEwpnh"
      },
      "source": [
        "## LangGraph Powered RAG\n",
        "\n",
        "First and foremost, LangChain provides a convenient way to store our chunks and their embeddings.\n",
        "\n",
        "It's called a `VectorStore`!\n",
        "\n",
        "We'll be using QDrant as our `VectorStore` today. You can read more about it [here](https://qdrant.tech/documentation/).\n",
        "\n",
        "Think of a `VectorStore` as a smart way to house your chunks and their associated embedding vectors. The implementation of the `VectorStore` also allows for smarter and more efficient search of our embedding vectors - as the method we used above would not scale well as we got into the millions of chunks.\n",
        "\n",
        "Otherwise, the process remains relatively similar under the hood!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBErqPRgxgZR"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We'll be leveraging the `DirectoryLoader` to load our PDFs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AHA9L3Jxo3r",
        "outputId": "c3535941-ce13-4ddd-ef08-b6b4ab1e507b"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "directory_loader = DirectoryLoader(\"data\", glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "\n",
        "loan_knowledge_resources = directory_loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH7ZPVJLx6Cn"
      },
      "source": [
        "### Chunking Our Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSCRQUSyBKT"
      },
      "source": [
        "Let's do the same process as we did before with our `RecursiveCharacterTextSplitter` - but this time we'll use ~200 tokens as our max chunk size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SzolG1FLx2f_"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
        "        text,\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")\n",
        "loan_knowledge_chunks = text_splitter.split_documents(loan_knowledge_resources)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpV4f1eXyXVJ",
        "outputId": "a666d551-6b20-4761-82ad-6f9b524d1660"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(loan_knowledge_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJ60Ck6ybe_"
      },
      "source": [
        "Let's verify the process worked as intended by checking our max document length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "950mB338yZR8",
        "outputId": "9813bfb9-cf80-4787-c962-69a5b1ccdff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "735\n"
          ]
        }
      ],
      "source": [
        "max_chunk_length = 0\n",
        "\n",
        "for chunk in loan_knowledge_chunks:\n",
        "  max_chunk_length = max(max_chunk_length, tiktoken_len(chunk.page_content))\n",
        "\n",
        "print(max_chunk_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDt3RufQy1cP"
      },
      "source": [
        "Perfect! Now we can carry on to creating and storing our embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocCe4zLy5qT"
      },
      "source": [
        "### Embeddings and Vector Storage\n",
        "\n",
        "We'll use the `text-embedding-3-small` embedding model again - and `Qdrant` to store all our embedding vectors for easy retrieval later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7M0X1eVlWPFf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    documents=loan_knowledge_chunks,\n",
        "    embedding=embedding_model,\n",
        "    location=\":memory:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NDvjfzXhVp"
      },
      "source": [
        "Now let's set up our retriever, just as we saw before, but this time using LangChain's simple `as_retriever()` method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Edjx19YBXavZ"
      },
      "outputs": [],
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OM0DiYcOj-"
      },
      "source": [
        "#### Back to the Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7apXaEBzQai"
      },
      "source": [
        "We're ready to move to the next step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMhcU37dzV5k"
      },
      "source": [
        "### Setting up our RAG\n",
        "\n",
        "We'll use the same LangGraph pipeline we created in the first notebook. \n",
        "\n",
        "Let's think through each part:\n",
        "\n",
        "1. First we need to retrieve context\n",
        "2. We need to pipe that context to our model\n",
        "3. We need to parse that output\n",
        "\n",
        "Let's start by setting up our prompt again, just so it's fresh in our minds!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oatgDa7cOXWV"
      },
      "source": [
        "#### üèóÔ∏è Activity #2:\n",
        "\n",
        "Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TE5tick_YPJj"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "HUMAN_TEMPLATE = \"\"\"\n",
        "#CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{query}\n",
        "\n",
        "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it's not contained in the provided context respond with \"I don't know\"\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", HUMAN_TEMPLATE)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI2tNXIT1iuB"
      },
      "source": [
        "We'll set our Generator - `gpt-4.1-nano` in this case - below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rZ-9gF1x1iEz"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZKadufhc2RL"
      },
      "source": [
        "#### Our RAG Application\n",
        "\n",
        "Let's spin up the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VnGthXpzzo-R"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "class State(TypedDict):\n",
        "  question: str\n",
        "  context: list[Document]\n",
        "  response: str\n",
        "\n",
        "def retrieve(state: State) -> State:\n",
        "  retrieved_docs = qdrant_retriever.invoke(state[\"question\"])\n",
        "  return {\"context\" : retrieved_docs}\n",
        "\n",
        "def generate(state: State) -> State:\n",
        "  generator_chain = chat_prompt | openai_chat_model | StrOutputParser()\n",
        "  response = generator_chain.invoke({\"query\" : state[\"question\"], \"context\" : state[\"context\"]})\n",
        "  return {\"response\" : response}\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder = graph_builder.add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "rag_graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0KAPrtFMtRd"
      },
      "source": [
        "Let's get a visual understanding of our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ocIXNGMsue",
        "outputId": "eb34fc03-7052-4825-82e1-85489ac84e78"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x10ddc66c0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0C5CFRHOxtB"
      },
      "source": [
        "Let's test our chain out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JSDyVefDaue4"
      },
      "outputs": [],
      "source": [
        "response = rag_graph.invoke({\"question\" : \"Is applying for and securing a student loan in 2025 a terrible idea?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "yfEAoG3HLC3J",
        "outputId": "e1633e4f-5405-46c3-f344-ffc324ac69d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I don't know\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FSZFdCM5LFoq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            "Available at SSRN: https://ssrn.com/abstract=\n",
            "5232910 or http://dx.doi.org/10.2139/ssrn.5232910.\n",
            "38\n",
            "----\n",
            "Context:\n",
            "Liu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio\n",
            "Petroni, and Per\n",
            "----\n",
            "Context:\n",
            "References\n",
            "Acemoglu, Daron, ‚ÄúThe Simple Macroeconomics of AI,‚Äù Technical Report 32487, National Bure\n",
            "----\n",
            "Context:\n",
            "Garicano, Luis, ‚ÄúHierarchies and the Organization of Knowledge in Production,‚Äù Journal of Political\n",
            "\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "for context in response[\"context\"]:\n",
        "  print(\"Context:\")\n",
        "  print(context.page_content[:100])\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nagiJ6l6noPL"
      },
      "source": [
        "Let's see if it can handle a query that is totally unrelated to the source documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HOd2nJKZnsty"
      },
      "outputs": [],
      "source": [
        "response = rag_graph.invoke({\"question\" : \"What is the airspeed velocity of an unladen swallow?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "TmLCKNGZLTh6",
        "outputId": "1478d061-7129-4b14-c717-c99d130a9488"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I don't know\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtSdDsXZXam"
      },
      "source": [
        "## Task 3: Setting Up LangSmith\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_TRACING_v2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com/\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com/\"\n",
        "\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = f\"LangSmith - {unique_id}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms4msyKLaIr6"
      },
      "source": [
        "### LangSmith API\n",
        "\n",
        "In order to use LangSmith - you will need an API key. You can sign up for a free account on [LangSmith's homepage!](https://www.langchain.com/langsmith)\n",
        "\n",
        "Once you have created your account, Take the navigation option for `Settings` then `API Keys` to create an API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVq1EYngEMhV",
        "outputId": "5560a787-8114-4c72-b95a-6a776043427a"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qy0MMBLacXv"
      },
      "source": [
        "Let's test our our first generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eoqBtBQERXP",
        "outputId": "fc1ea857-6784-4e7e-9fd4-94fbdd9f693a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I don't know\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_graph.invoke({\"question\" : \"What is the maximum loan amount I can get from the government to go to school these days?\"}, {\"tags\" : [\"Demo Run\"]})['response']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxABFzPr2ny"
      },
      "source": [
        "## Task 4: Examining the Trace in LangSmith!\n",
        "\n",
        "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52o58AfsLK6"
      },
      "source": [
        "#### üèóÔ∏è Activity #1:\n",
        "\n",
        "Include a screenshot of your trace and explain what it means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLxh0-thanXt"
      },
      "source": [
        "## Task 5: Loading Our Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsJ8uCbT7S1Z",
        "outputId": "cdcbe030-aae2-46da-8cb7-30811da9f87d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'DataRepository'...\n",
            "remote: Enumerating objects: 122, done.\u001b[K\n",
            "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 122 (delta 37), reused 39 (delta 10), pack-reused 8 (from 1)\u001b[K\n",
            "Receiving objects: 100% (122/122), 78.04 MiB | 24.24 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AI-Maker-Space/DataRepository.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "w5fmy5Gy7X03"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv(\"DataRepository/student_loan_rag_test_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBtPs7p9Mbz"
      },
      "source": [
        "Now we can set up our LangSmith client - and we'll add the above created dataset to our LangSmith instance!\n",
        "\n",
        "> NOTE: Read more about this process [here](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-list-of-values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "T9exE2e6F3gF"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"langsmith-student-loan-rag-v0\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"Student Loan RAG Test Questions\"\n",
        ")\n",
        "\n",
        "for triplet in test_df.iterrows():\n",
        "  triplet = triplet[1]\n",
        "  client.create_example(\n",
        "      inputs={\"question\" : triplet[\"question\"], \"context\": triplet[\"context\"]},\n",
        "      outputs={\"answer\" : triplet[\"answer\"]},\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgi14vSbFIc"
      },
      "source": [
        "## Task 6: Evaluation\n",
        "\n",
        "Now we can run the evaluation!\n",
        "\n",
        "We'll need to start by preparing some custom data preparation functions to ensure our chain works with the expected inputs/outputs from the `evaluate` process in LangSmith.\n",
        "\n",
        "> NOTE: More reading on this available [here](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fbjnv3bMwQKg"
      },
      "outputs": [],
      "source": [
        "def prepare_data_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.outputs[\"answer\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_data_noref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_context_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.inputs[\"context\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuwnMdtl9nwz"
      },
      "source": [
        "We'll be using a few custom evaluators to evaluate our pipeline, as well as a few \"built in\" methods!\n",
        "\n",
        "Check out the built-ins [here](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "501c5e204387460e8e9a585c9b2ca834",
            "323a077888c84bdfbac393d1ebef9d6c",
            "710fe93f3241401bafa10a1e0a1bda02",
            "c65a6921701742da9b67c591b19b6336",
            "17d4193e78a14050a11c5f8306b3ba0b",
            "99751d16888640078ae4820cacab5760",
            "a8f5f4beaa8f4a3b94002c5ed122c2a7",
            "54e7ccad2e774261a3313316a1397f66",
            "41e287b8dc674f839b6485c9606207d5",
            "f7457f38948d472da4027ba1566b47e4",
            "af61b863014d4a8c960570de31a02b3f"
          ]
        },
        "id": "CENtd4K_IQa3",
        "outputId": "872581e4-9c21-414f-d25e-9454c47c0587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Base RAG Evaluation-cf677585' at:\n",
            "https://smith.langchain.com/o/7ffaf126-290e-4d08-9a81-6ef0b42d5153/datasets/3d07c4d3-542b-484a-b3a4-3b98025810a5/compare?selectedSessions=974fa7f2-6f12-4329-87e8-2f53ba5bedcb\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "938cb2e3f93d4341b23d3e8aea962496",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "eval_llm = ChatOpenAI(model=\"gpt-4o-mini\", tags=[\"eval_llm\"])\n",
        "\n",
        "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\",  config={\"llm\":eval_llm}, prepare_data=prepare_context_ref)\n",
        "\n",
        "unlabeled_dopeness_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\" : {\n",
        "            \"dopeness\" : \"Is the answer to the question dope, meaning cool - awesome - and legit?\"\n",
        "        },\n",
        "        \"llm\" : eval_llm,\n",
        "    },\n",
        "    prepare_data=prepare_data_noref\n",
        ")\n",
        "\n",
        "labeled_score_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_score_string\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"accuracy\": \"Is the generated answer the same as the reference answer?\"\n",
        "        },\n",
        "    },\n",
        "    prepare_data=prepare_data_ref\n",
        ")\n",
        "\n",
        "base_rag_results = evaluate(\n",
        "    rag_graph.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        ],\n",
        "    experiment_prefix=\"Base RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPocfrNFiYWi"
      },
      "source": [
        "#### ‚ùìQuestion #1:\n",
        "\n",
        "What conclusions can you draw about the above results?\n",
        "\n",
        "Describe in your own words what the metrics are expressing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
