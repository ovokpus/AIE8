{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ollama Setup and Testing\n",
        "\n",
        "This notebook will help you set up and test Ollama with LangChain connectors before starting the main RAG assignment.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **Install Ollama** from https://ollama.ai\n",
        "   - On Linux/Mac: `curl https://ollama.ai/install.sh | sh`\n",
        "   - On Windows: Download and run the installer\n",
        "\n",
        "2. **Verify Installation**\n",
        "   - Run `ollama -v` in your terminal\n",
        "   - Should show version 0.11.10 or greater\n",
        "\n",
        "3. **Pull Required Models**\n",
        "   ```bash\n",
        "   # For chat/inference\n",
        "   ollama pull gpt-oss:20b\n",
        "   \n",
        "   # For embeddings\n",
        "   ollama pull embeddinggemma:latest\n",
        "   ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Test Ollama Connection\n",
        "\n",
        "First, let's verify that Ollama is running and accessible:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Ollama is running!\n",
            "\n",
            "Available models:\n",
            "  - embeddinggemma:latest\n",
            "  - gpt-oss:20b\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Test if Ollama is running\n",
        "try:\n",
        "    response = requests.get('http://localhost:11434/api/tags')\n",
        "    if response.status_code == 200:\n",
        "        models = json.loads(response.text)\n",
        "        print(\"✅ Ollama is running!\")\n",
        "        print(\"\\nAvailable models:\")\n",
        "        for model in models.get('models', []):\n",
        "            print(f\"  - {model['name']}\")\n",
        "    else:\n",
        "        print(\"❌ Ollama is not responding properly\")\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"❌ Cannot connect to Ollama. Make sure it's running!\")\n",
        "    print(\"Start Ollama by running 'ollama serve' in a terminal\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Test Embeddings with Ollama\n",
        "\n",
        "Now let's test creating embeddings using the LangChain Ollama connector:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embedding model initialized\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = OllamaEmbeddings(\n",
        "    model=\"embeddinggemma:latest\",\n",
        "    base_url=\"http://localhost:11434\"  # Default Ollama URL\n",
        ")\n",
        "\n",
        "print(\"✅ Embedding model initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding query: 'What is the meaning of life?'\n",
            "\n",
            "✅ Successfully created embedding!\n",
            "Embedding dimension: 768\n",
            "First 10 values: [-0.14624307, 0.029132523, 0.037615955, -0.02487349, -0.02655731, 0.016060056, -0.027486855, 0.027256342, 0.01139732, -2.40085e-05]\n"
          ]
        }
      ],
      "source": [
        "# Test embedding a single query\n",
        "test_query = \"What is the meaning of life?\"\n",
        "\n",
        "print(f\"Embedding query: '{test_query}'\")\n",
        "embedding = embedding_model.embed_query(test_query)\n",
        "\n",
        "print(f\"\\n✅ Successfully created embedding!\")\n",
        "print(f\"Embedding dimension: {len(embedding)}\")\n",
        "print(f\"First 10 values: {embedding[:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding multiple documents...\n",
            "\n",
            "✅ Successfully created 3 embeddings!\n",
            "\n",
            "Document 1: 'The quick brown fox jumps over the lazy dog....'\n",
            "  Embedding dimension: 768\n",
            "  First 5 values: [-0.14781645, 0.002890088, 0.052144155, -0.029089162, -0.036695607]\n",
            "\n",
            "Document 2: 'Machine learning is a subset of artificial intelli...'\n",
            "  Embedding dimension: 768\n",
            "  First 5 values: [-0.1240763, -0.0027511106, -0.00032809668, 0.010076388, 0.001677464]\n",
            "\n",
            "Document 3: 'Python is a popular programming language for data ...'\n",
            "  Embedding dimension: 768\n",
            "  First 5 values: [-0.16269195, -0.010295422, 0.025464684, 0.000692403, -0.01887165]\n"
          ]
        }
      ],
      "source": [
        "# Test embedding multiple documents\n",
        "test_documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Python is a popular programming language for data science.\"\n",
        "]\n",
        "\n",
        "print(\"Embedding multiple documents...\")\n",
        "embeddings = embedding_model.embed_documents(test_documents)\n",
        "\n",
        "print(f\"\\n✅ Successfully created {len(embeddings)} embeddings!\")\n",
        "for i, doc in enumerate(test_documents):\n",
        "    print(f\"\\nDocument {i+1}: '{doc[:50]}...'\")\n",
        "    print(f\"  Embedding dimension: {len(embeddings[i])}\")\n",
        "    print(f\"  First 5 values: {embeddings[i][:5]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Test Model Inference with Ollama\n",
        "\n",
        "Now let's test using Ollama for text generation/inference using the LangChain connector:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Chat model initialized\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Initialize the chat model\n",
        "chat_model = ChatOllama(\n",
        "    model=\"gpt-oss:20b\",\n",
        "    temperature=0.7,\n",
        "    base_url=\"http://localhost:11434\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"✅ Chat model initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Let's add a procedure to measure the inference performance of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detailed_performance_metrics(response_metadata):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive performance metrics from Ollama response metadata\n",
        "    \"\"\"\n",
        "    # Extract all timing data (in nanoseconds)\n",
        "    total_duration = response_metadata.get('total_duration', 0)\n",
        "    load_duration = response_metadata.get('load_duration', 0)\n",
        "    prompt_eval_duration = response_metadata.get('prompt_eval_duration', 0)\n",
        "    eval_duration = response_metadata.get('eval_duration', 0)\n",
        "    \n",
        "    # Extract token counts\n",
        "    prompt_eval_count = response_metadata.get('prompt_eval_count', 0)\n",
        "    eval_count = response_metadata.get('eval_count', 0)\n",
        "    \n",
        "    # Convert to seconds\n",
        "    total_seconds = total_duration / 1_000_000_000\n",
        "    load_seconds = load_duration / 1_000_000_000\n",
        "    prompt_eval_seconds = prompt_eval_duration / 1_000_000_000\n",
        "    eval_seconds = eval_duration / 1_000_000_000\n",
        "    \n",
        "    # tokens per second\n",
        "    tokens_per_second = eval_count / eval_seconds\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'generation_tokens_per_second': eval_count / eval_seconds if eval_seconds > 0 else 0,\n",
        "        'prompt_tokens_per_second': prompt_eval_count / prompt_eval_seconds if prompt_eval_seconds > 0 else 0,\n",
        "        'total_tokens': prompt_eval_count + eval_count,\n",
        "        'total_time_seconds': total_seconds,\n",
        "        'load_time_seconds': load_seconds,\n",
        "        'generation_time_seconds': eval_seconds,\n",
        "        'prompt_processing_time_seconds': prompt_eval_seconds,\n",
        "        }\n",
        "\n",
        "    print(f\"Total tokens: {metrics['total_tokens']}\")\n",
        "    print(f\"Total time seconds: {metrics['total_time_seconds']}\")\n",
        "    print(f\"Load time seconds: {metrics['load_time_seconds']}\")\n",
        "    print(f\"Generation time seconds: {metrics['generation_time_seconds']}\")\n",
        "    print(f\"Prompt processing time seconds: {metrics['prompt_processing_time_seconds']}\")\n",
        "    print(f\"Generation tokens per second: {metrics['generation_tokens_per_second']}\")\n",
        "    print(f\"Prompt tokens per second: {metrics['prompt_tokens_per_second']}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Explain quantum computing in one sentence.\n",
            "\n",
            "Generating response...\n",
            "\n",
            "✅ Response generated!\n",
            "\n",
            "Model output: Quantum computing uses qubits that can exist in multiple states at once and become entangled, allowing certain calculations—such as factoring large numbers or simulating quantum systems—to be performed far more efficiently than with classical bits.\n"
          ]
        }
      ],
      "source": [
        "# Test simple inference\n",
        "prompt = \"Explain quantum computing in one sentence.\"\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(\"\\nGenerating response...\")\n",
        "\n",
        "response = chat_model.invoke(prompt)\n",
        "\n",
        "print(f\"\\n✅ Response generated!\")\n",
        "print(f\"\\nModel output: {response.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sending messages to model...\n",
            "\n",
            "✅ Response generated!\n",
            "\n",
            "Model output: **Machine learning (ML)** is a way to let computers learn patterns and make decisions on their own—without being written out for every single step.\n",
            "\n",
            "---\n",
            "\n",
            "### Think of it like this\n",
            "\n",
            "| What you usually do | What a machine‑learning model does |\n",
            "|---------------------|-------------------------------------|\n",
            "| You hand‑craft a recipe: “If the temperature is above 30 °C, turn on the fan.” | The model looks at lots of past data (temperature, fan usage, etc.) and automatically figures out the rule that works best. |\n",
            "\n",
            "---\n",
            "\n",
            "### How it works in a nutshell\n",
            "\n",
            "1. **Collect data** – Gather examples (images, numbers, text, etc.).  \n",
            "2. **Teach the model** – Feed the data into an algorithm that adjusts its internal parameters (like the weights in a neural network).  \n",
            "3. **Make predictions** – Once trained, the model can classify new data or predict future outcomes.\n",
            "\n",
            "---\n",
            "\n",
            "### Common types\n",
            "\n",
            "| Type | What it’s good for | Example |\n",
            "|------|--------------------|---------|\n",
            "| **Supervised learning** | Predicting a known outcome (e.g., spam detection). | Classifying emails as spam or not spam. |\n",
            "| **Unsupervised learning** | Finding hidden structure (e.g., grouping customers). | Segmenting shoppers by buying habits. |\n",
            "| **Reinforcement learning** | Learning by trial‑and‑error (e.g., playing games). | A robot learning to navigate a maze. |\n",
            "\n",
            "---\n",
            "\n",
            "### Everyday examples\n",
            "\n",
            "| Task | ML in action |\n",
            "|------|--------------|\n",
            "| **Image search** | Recognizes objects in photos. |\n",
            "| **Voice assistants** | Understands spoken commands. |\n",
            "| **Recommender systems** | Suggests movies or products. |\n",
            "| **Spam filters** | Blocks unwanted emails. |\n",
            "\n",
            "---\n",
            "\n",
            "### Bottom line\n",
            "\n",
            "Machine learning lets computers **learn from data** instead of being explicitly programmed for every scenario. The more relevant data you give it, the smarter it can become—often discovering patterns humans might miss.\n"
          ]
        }
      ],
      "source": [
        "# Test with system message and human message\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful AI assistant that explains complex topics simply.\"),\n",
        "    HumanMessage(content=\"What is machine learning?\")\n",
        "]\n",
        "\n",
        "print(\"Sending messages to model...\")\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "print(f\"\\n✅ Response generated!\")\n",
        "print(f\"\\nModel output: {response.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 548\n",
            "Total time seconds: 9.898785125\n",
            "Load time seconds: 0.08863\n",
            "Generation time seconds: 9.526577042\n",
            "Prompt processing time seconds: 0.243859\n",
            "Generation tokens per second: 47.86609062096745\n",
            "Prompt tokens per second: 377.2671912867682\n"
          ]
        }
      ],
      "source": [
        "_ = detailed_performance_metrics(response.response_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Test Streaming Response\n",
        "\n",
        "Ollama supports streaming responses, which is useful for real-time applications:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Write a haiku about artificial intelligence.\n",
            "\n",
            "Streaming response:\n",
            "----------------------------------------\n",
            "Silent code hums now  \n",
            "Mind forged from endless data  \n",
            "Stars in circuits glow.\n",
            "----------------------------------------\n",
            "\n",
            "✅ Streaming completed!\n"
          ]
        }
      ],
      "source": [
        "# Test streaming\n",
        "prompt = \"Write a haiku about artificial intelligence.\"\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(\"\\nStreaming response:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for chunk in chat_model.stream(prompt):\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"\\n✅ Streaming completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "If all the tests above passed, you're ready to use Ollama with LangChain! Here's what we tested:\n",
        "\n",
        "✅ **Embeddings**: \n",
        "- Created embeddings for single queries\n",
        "- Created embeddings for multiple documents\n",
        "- Verified embedding dimensions\n",
        "\n",
        "✅ **Model Inference**:\n",
        "- Simple text generation\n",
        "- Chat with system and human messages\n",
        "- Streaming responses\n",
        "- Integration with LangChain chains\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "If you encounter issues:\n",
        "\n",
        "1. **Model Not Found**: Pull the required models (`ollama pull <model-name>`)\n",
        "2. **Slow Performance**: Ollama models run on CPU by default. For better performance:\n",
        "   - Use smaller models for testing\n",
        "   - Consider GPU acceleration if available\n",
        "3. **Memory Issues**: Large models require significant RAM. Try smaller variants if needed.\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Now you're ready to proceed with the main RAG assignment using Ollama!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
