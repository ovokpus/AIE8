{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ollama Setup and Testing\n",
        "\n",
        "This notebook will help you set up and test Ollama with LangChain connectors before starting the main RAG assignment.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **Install Ollama** from https://ollama.ai\n",
        "   - On Linux/Mac: `curl https://ollama.ai/install.sh | sh`\n",
        "   - On Windows: Download and run the installer\n",
        "\n",
        "2. **Verify Installation**\n",
        "   - Run `ollama -v` in your terminal\n",
        "   - Should show version 0.11.10 or greater\n",
        "\n",
        "3. **Pull Required Models**\n",
        "   ```bash\n",
        "   # For chat/inference\n",
        "   ollama pull gpt-oss:20b\n",
        "   \n",
        "   # For embeddings\n",
        "   ollama pull embeddinggemma:latest\n",
        "   ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Test Ollama Connection\n",
        "\n",
        "First, let's verify that Ollama is running and accessible:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Ollama is running!\n",
            "\n",
            "Available models:\n",
            "  - embeddinggemma:latest\n",
            "  - gpt-oss:20b\n",
            "  - deepseek-r1:latest\n",
            "  - mxbai-embed-large:latest\n",
            "  - deepseek-r1:8b\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Test if Ollama is running\n",
        "try:\n",
        "    response = requests.get('http://localhost:11434/api/tags')\n",
        "    if response.status_code == 200:\n",
        "        models = json.loads(response.text)\n",
        "        print(\"✅ Ollama is running!\")\n",
        "        print(\"\\nAvailable models:\")\n",
        "        for model in models.get('models', []):\n",
        "            print(f\"  - {model['name']}\")\n",
        "    else:\n",
        "        print(\"❌ Ollama is not responding properly\")\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"❌ Cannot connect to Ollama. Make sure it's running!\")\n",
        "    print(\"Start Ollama by running 'ollama serve' in a terminal\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Test Embeddings with Ollama\n",
        "\n",
        "Now let's test creating embeddings using the LangChain Ollama connector:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embedding model initialized\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = OllamaEmbeddings(\n",
        "    model=\"embeddinggemma:latest\",\n",
        "    base_url=\"http://localhost:11434\"  # Default Ollama URL\n",
        ")\n",
        "\n",
        "print(\"✅ Embedding model initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding query: 'What is the meaning of life?'\n",
            "\n",
            "✅ Successfully created embedding!\n",
            "Embedding dimension: 768\n",
            "First 10 values: [-0.14612523, 0.029183429, 0.037420813, -0.02506321, -0.026125213, 0.016262747, -0.026964674, 0.026944311, 0.0111782, -4.2271626e-05]\n"
          ]
        }
      ],
      "source": [
        "# Test embedding a single query\n",
        "test_query = \"What is the meaning of life?\"\n",
        "\n",
        "print(f\"Embedding query: '{test_query}'\")\n",
        "embedding = embedding_model.embed_query(test_query)\n",
        "\n",
        "print(f\"\\n✅ Successfully created embedding!\")\n",
        "print(f\"Embedding dimension: {len(embedding)}\")\n",
        "print(f\"First 10 values: {embedding[:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding multiple documents...\n",
            "\n",
            "✅ Successfully created 3 embeddings!\n",
            "\n",
            "Document 1: 'The quick brown fox jumps over the lazy dog....'\n",
            "  Embedding dimension: 768\n",
            "  First 5 values: [-0.14769432, 0.0027910522, 0.05211773, -0.028957522, -0.03694144]\n",
            "\n",
            "Document 2: 'Machine learning is a subset of artificial intelli...'\n",
            "  Embedding dimension: 768\n",
            "  First 5 values: [-0.12411143, -0.0027318636, -0.00045949276, 0.009990234, 0.0016487375]\n",
            "\n",
            "Document 3: 'Python is a popular programming language for data ...'\n",
            "  Embedding dimension: 768\n",
            "  First 5 values: [-0.16281867, -0.010769529, 0.025246046, 0.0003698215, -0.018286267]\n"
          ]
        }
      ],
      "source": [
        "# Test embedding multiple documents\n",
        "test_documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Python is a popular programming language for data science.\"\n",
        "]\n",
        "\n",
        "print(\"Embedding multiple documents...\")\n",
        "embeddings = embedding_model.embed_documents(test_documents)\n",
        "\n",
        "print(f\"\\n✅ Successfully created {len(embeddings)} embeddings!\")\n",
        "for i, doc in enumerate(test_documents):\n",
        "    print(f\"\\nDocument {i+1}: '{doc[:50]}...'\")\n",
        "    print(f\"  Embedding dimension: {len(embeddings[i])}\")\n",
        "    print(f\"  First 5 values: {embeddings[i][:5]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Test Model Inference with Ollama\n",
        "\n",
        "Now let's test using Ollama for text generation/inference using the LangChain connector:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Chat model initialized\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Initialize the chat model\n",
        "chat_model = ChatOllama(\n",
        "    model=\"gpt-oss:20b\",\n",
        "    temperature=0.7,\n",
        "    base_url=\"http://localhost:11434\"\n",
        ")\n",
        "\n",
        "print(\"✅ Chat model initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Explain quantum computing in one sentence.\n",
            "\n",
            "Generating response...\n",
            "\n",
            "✅ Response generated!\n",
            "\n",
            "Model output: Quantum computing harnesses quantum bits that can exist in multiple states at once, enabling it to solve certain problems exponentially faster than classical computers.\n"
          ]
        }
      ],
      "source": [
        "# Test simple inference\n",
        "prompt = \"Explain quantum computing in one sentence.\"\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(\"\\nGenerating response...\")\n",
        "\n",
        "response = chat_model.invoke(prompt)\n",
        "\n",
        "print(f\"\\n✅ Response generated!\")\n",
        "print(f\"\\nModel output: {response.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sending messages to model...\n",
            "\n",
            "✅ Response generated!\n",
            "\n",
            "Model output: **Machine learning is a way of teaching computers to learn from experience, just like people do.**\n",
            "\n",
            "---\n",
            "\n",
            "### 1. The big idea\n",
            "- **Instead of writing every rule by hand**, we give a computer a lot of data and let it discover the patterns itself.\n",
            "- The computer builds a *model* (a mathematical recipe) that can then make predictions or decisions on new, unseen data.\n",
            "\n",
            "### 2. How it works in plain terms\n",
            "\n",
            "| Step | What happens | Simple example |\n",
            "|------|--------------|----------------|\n",
            "| **Collect data** | Gather lots of examples. | Photos of cats and dogs. |\n",
            "| **Choose a task** | Decide what you want the model to do. | Tell the computer “label each photo as cat or dog.” |\n",
            "| **Teach the model** | Let the computer look at the data and adjust its internal settings to fit the examples. | The computer adjusts weights in a neural network so that it correctly identifies cats. |\n",
            "| **Test it** | Give it new data it hasn’t seen before to see how well it works. | Show it a new photo of a cat; it should say “cat.” |\n",
            "| **Use it** | Deploy the model in a real application. | An app that automatically tags your photos. |\n",
            "\n",
            "---\n",
            "\n",
            "### 3. Types of learning (the “families”)\n",
            "\n",
            "| Type | How it learns | Typical use |\n",
            "|------|---------------|-------------|\n",
            "| **Supervised** | Learns from labeled examples (input + correct output). | Spam detection, image classification. |\n",
            "| **Unsupervised** | Finds patterns or groups without labels. | Customer segmentation, anomaly detection. |\n",
            "| **Reinforcement** | Learns by trial‑and‑error, getting rewards for good actions. | Game AI, robotics, recommendation engines. |\n",
            "\n",
            "---\n",
            "\n",
            "### 4. Why it matters\n",
            "\n",
            "- **Automation**: Tasks that used to need human experts can now be done automatically (e.g., diagnosing diseases from scans).\n",
            "- **Personalization**: Services adapt to each user (movie recommendations, smart assistants).\n",
            "- **Insight**: Machines can spot hidden patterns in huge data sets that humans might miss.\n",
            "\n",
            "---\n",
            "\n",
            "### 5. A quick analogy\n",
            "\n",
            "Think of a toddler learning to recognize a ball:\n",
            "\n",
            "1. **Show**: The toddler sees many balls and hears “ball” spoken.\n",
            "2. **Guess**: The toddler says “ball” when seeing a round object.\n",
            "3. **Correct**: The parent says “yes” or “no” to help refine the guess.\n",
            "4. **Repeat**: Over time, the toddler becomes good at identifying balls, even in new settings.\n",
            "\n",
            "Machine learning is the computer version of that process—just with far more data and a lot faster.\n",
            "\n",
            "---\n",
            "\n",
            "**Bottom line:** Machine learning lets computers improve their performance on a task by learning from data, without being explicitly programmed for every detail. It’s the engine behind things like voice assistants, recommendation sites, and autonomous cars.\n"
          ]
        }
      ],
      "source": [
        "# Test with system message and human message\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful AI assistant that explains complex topics simply.\"),\n",
        "    HumanMessage(content=\"What is machine learning?\")\n",
        "]\n",
        "\n",
        "print(\"Sending messages to model...\")\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "print(f\"\\n✅ Response generated!\")\n",
        "print(f\"\\nModel output: {response.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Test Streaming Response\n",
        "\n",
        "Ollama supports streaming responses, which is useful for real-time applications:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Write a haiku about artificial intelligence.\n",
            "\n",
            "Streaming response:\n",
            "----------------------------------------\n",
            "Silent code breathes deep  \n",
            "Neural nets hum quietly  \n",
            "Future breathes in code\n",
            "----------------------------------------\n",
            "\n",
            "✅ Streaming completed!\n"
          ]
        }
      ],
      "source": [
        "# Test streaming\n",
        "prompt = \"Write a haiku about artificial intelligence.\"\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(\"\\nStreaming response:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for chunk in chat_model.stream(prompt):\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"\\n✅ Streaming completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "If all the tests above passed, you're ready to use Ollama with LangChain! Here's what we tested:\n",
        "\n",
        "✅ **Embeddings**: \n",
        "- Created embeddings for single queries\n",
        "- Created embeddings for multiple documents\n",
        "- Verified embedding dimensions\n",
        "\n",
        "✅ **Model Inference**:\n",
        "- Simple text generation\n",
        "- Chat with system and human messages\n",
        "- Streaming responses\n",
        "- Integration with LangChain chains\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "If you encounter issues:\n",
        "\n",
        "1. **Model Not Found**: Pull the required models (`ollama pull <model-name>`)\n",
        "2. **Slow Performance**: Ollama models run on CPU by default. For better performance:\n",
        "   - Use smaller models for testing\n",
        "   - Consider GPU acceleration if available\n",
        "3. **Memory Issues**: Large models require significant RAM. Try smaller variants if needed.\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Now you're ready to proceed with the main RAG assignment using Ollama!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
